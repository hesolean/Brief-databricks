# ğŸ’§ Pipeline Azure Databricks â€“ Analyse de la QualitÃ© de lâ€™Eau en France  

> Un pipeline qui coule de source ğŸ’§  
> Projet rÃ©alisÃ© dans le cadre dâ€™un brief dâ€™alternance en **Data Engineering**.

[![Build Status](https://github.com/hesolean/Brief-databricks/actions/workflows/release.yml/badge.svg)](https://github.com/<ton_user>/<ton_repo>/actions)
[![semantic-release](https://img.shields.io/badge/semantic--release-automated-brightgreen)](https://github.com/semantic-release/semantic-release)

---

## ğŸ“˜ Contexte du projet

Ce projet vise Ã  concevoir un **pipeline de donnÃ©es moderne** sur **Azure Databricks**, suivant lâ€™architecture **Medallion (Bronze â€“ Silver â€“ Gold)**, pour analyser la **qualitÃ© de lâ€™eau potable en France** Ã  partir des donnÃ©es publiques de [data.gouv.fr](https://www.data.gouv.fr).

Les contrÃ´les sanitaires de lâ€™**ANSES** (Agence Nationale de SÃ©curitÃ© Sanitaire) servent de base Ã  ce pipeline.  
Il permet dâ€™**ingÃ©rer**, **transformer** et **analyser** les rÃ©sultats de conformitÃ© de lâ€™eau distribuÃ©e commune par commune.

---

## ğŸ¯ Objectifs du projet

### Objectif principal
Construire un pipeline complet sur **Azure Databricks** avec **Delta Live Tables (DLT)** pour automatiser le traitement des donnÃ©es selon lâ€™architecture **Medallion**.

### Objectifs secondaires
- âš™ï¸ **Infrastructure Azure :** Configurer Azure Data Lake Storage Gen2 et Databricks  
- ğŸš° **Ingestion automatisÃ©e :** ImplÃ©menter DLT pour ingÃ©rer les donnÃ©es brutes  
- ğŸ” **CI/CD :** Mettre en place un workflow GitHub Actions avec **Semantic Release**  
- ğŸ§ª **QualitÃ© des donnÃ©es :** IntÃ©gration de **Great Expectations**  
- ğŸ“† **Orchestration :** Automatisation via Databricks Workflows  
- ğŸŒ **API :** *(prÃ©vu)* Exposition des donnÃ©es via lâ€™API Databricks  

> ğŸ’¡ Le projet est finalisÃ© jusquâ€™Ã  la mise en place de l'orchestration.  
> La derniÃ¨re Ã©tape est planifiÃ©e comme Ã©volution.

---

## ğŸ§± Structure du projet

```bash
pipeline-qualite-eau/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ ci.yml
â”‚       â””â”€â”€ release.yml
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 1-Import donnÃ©es source.ipynb
â”‚   â”œâ”€â”€ 2-1-Initier la couche bronze.ipynb
â”‚   â”œâ”€â”€ 2-2-Qualite donnÃ©es bronze.ipynb
â”‚   â””â”€â”€ 3-Initier la couche silver.ipynb
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .releaserc.json
â”œâ”€â”€ package.json
â”œâ”€â”€ CHANGELOG.md
â”œâ”€â”€ Brief Databrick.odt
â””â”€â”€ formation-data/
```
---

## ğŸ““ Description des notebooks
### 1ï¸âƒ£ Import donnÃ©es source

Ce notebook gÃ¨re lâ€™ingestion initiale des fichiers .txt depuis un Azure Blob Storage.
Les identifiants du stockage sont transmis au cluster via des variables dâ€™environnement pour Ã©viter toute exposition de secrets (voir captures d'Ã©cran Ã  la fin de Brief Databrick.odt).
RÃ©sultat : une table de travail brute, prÃªte Ã  Ãªtre exploitÃ©e.

### 2ï¸âƒ£ Initier la couche Bronze

Ce notebook crÃ©e la couche Bronze avec Delta Live Tables (DLT).
Les donnÃ©es y sont stockÃ©es telles quâ€™elles sont reÃ§ues, sans transformation, pour prÃ©server une trace intÃ©grale des sources.

### 2ï¸âƒ£ QualitÃ© donnÃ©es bronze

ce notebook teste les donnÃ©es de la table bronze et enregistre les rÃ©sultats dans une nouvelle table qualite_donnees_plv.

### 3ï¸âƒ£ Initier la couche Silver

Ce notebook dÃ©finit la couche Silver, oÃ¹ une colonne "conformitÃ©" est ajoutÃ©e :

- "non" si le texte "non conforme" est prÃ©sent dans conclusionprel,

- "oui" sinon.

Cette couche Ã  complÃ©ter, prÃ©pare les donnÃ©es pour la future analyse (couche Gold).

# Catalogue des Tables

Notre pipeline ETL organise les donnÃ©es en **trois couches** : **bronze**, **silver** et **qualitÃ©**, suivant le modÃ¨le classique Lakehouse. Chaque couche a un rÃ´le prÃ©cis dans la transformation et la valorisation des donnÃ©es.

## ğŸŸ« Bronze â€“ DonnÃ©es brutes
Les tables de la couche bronze contiennent les donnÃ©es **directement issues des fichiers TXT du Blob Azure**, sans transformation ni enrichissement. Elles servent de source primaire pour les couches supÃ©rieures.

| Table | Description |
|-------|-------------|
| `bronze_com` | DonnÃ©es brutes des fichiers `COM` |
| `bronze_plv` | DonnÃ©es brutes des fichiers `PLV` |

---

## ğŸŸ¦ Silver â€“ DonnÃ©es enrichies
La couche silver transforme et enrichit certaines donnÃ©es brutes pour les rendre **prÃªtes Ã  lâ€™usage analytique**.  

| Table | Description |
|-------|-------------|
| `silver_qualite_eau_conformite` | DonnÃ©es issues de `bronze_plv` avec ajout de la colonne **conformitÃ©**, indiquant si les enregistrements respectent les rÃ¨gles mÃ©tier |

---

## ğŸŸ© QualitÃ© â€“ DonnÃ©es prÃªtes pour exploitation
La couche qualitÃ© est **la cerise sur le gÃ¢teau** : elle regroupe les rÃ©sultats des **tests de qualitÃ© des donnÃ©es** exÃ©cutÃ©s via Great Expectations. Cette table est **immÃ©diatement exploitable** pour du reporting, par exemple dans Power BI.

| Table | Description |
|-------|-------------|
| `qualite_donnees_plv` | RÃ©sultats dÃ©taillÃ©s des validations qualitÃ©, avec la date de test et la conformitÃ© par colonne |

---

## ğŸ’¡ Flux global des donnÃ©es

```bash
TXT Azure Blob
â”‚
â–¼
Bronze (bronze_com, bronze_plv)
â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Silver (silver_qualite_eau_conformite)
â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º QualitÃ© (qualite_donnees_plv) â†’ Reporting / Power BI
```

Chaque table est **cumulable** et permet de tracer le chemin des donnÃ©es depuis la source brute jusquâ€™aux indicateurs de qualitÃ© exploitable.

## ğŸ” IntÃ©gration Continue & Semantic Release

Une pipeline GitHub Actions est configurÃ©e pour gÃ©rer automatiquement les versions du projet via Semantic Release.

### âš™ï¸ Fonctionnement

Workflow : .github/workflows/release.yml

DÃ©clenchement : Ã  chaque push sur main ou dev

Versioning automatique : basÃ© sur le format Conventional Commits (feat:, fix:, docs:, etc.)

Changelog gÃ©nÃ©rÃ© : mise Ã  jour automatique de CHANGELOG.md

Tag Git : crÃ©Ã© automatiquement Ã  chaque release

### ğŸ§© Commande locale
npx semantic-release

### âœ… Avantages

Gestion de version automatisÃ©e et cohÃ©rente

Changelog toujours Ã  jour

Transparence sur lâ€™Ã©volution du projet

Compatible avec les workflows de dÃ©ploiement CI/CD

## ğŸ§© Orchestration du pipeline

Lâ€™orchestration du pipeline est assurÃ©e directement dans Databricks Workflows âš™ï¸
Un job a Ã©tÃ© configurÃ© pour enchaÃ®ner automatiquement les notebooks :
1ï¸âƒ£ Import donnÃ©es source â†’
2ï¸âƒ£ Initier la couche Bronze â†’
3ï¸âƒ£ Qualite des donnÃ©es de la couche bronze / Initier la couche Silver

Chaque Ã©tape est exÃ©cutÃ©e dans lâ€™ordre, garantissant la mise Ã  jour cohÃ©rente des tables et la traÃ§abilitÃ© complÃ¨te du flux.

ğŸª¶ Simple, efficace, et parfaitement intÃ©grÃ© Ã  lâ€™Ã©cosystÃ¨me Databricks : un pipeline qui sâ€™exÃ©cute tout seul, pendant que le cafÃ© infuse â˜•

## ğŸ§  Environnement technique
Outil / Service	RÃ´le
Azure Databricks	        ExÃ©cution des notebooks et pipelines
Azure Data Lake Gen2	    Stockage des donnÃ©es brutes
DLT (Delta Live Tables)	    Gestion automatisÃ©e des tables Bronze / Silver
GitHub Actions	            IntÃ©gration continue et automatisation des versions
Semantic Release	        Gestion sÃ©mantique du versioning et changelog
Great Expectation           Gestion des tests sur la qualitÃ© des donnÃ©es

## ğŸ“¸ Documentation et suivi

Le fichier Brief Databrick.odt contient :

- un sommaire,
- des captures dâ€™Ã©cran de toute l'architecture Azure et Databricks,
- la configuration des clusters Databricks,
- des notes de travail sur le dÃ©roulÃ© du projet,
- et des captures dâ€™Ã©cran de toute l'architecture Azure et Databricks.

## ğŸš€ Ã‰volutions prÃ©vues

CrÃ©ation de la couche Gold (donnÃ©es prÃªtes pour la visualisation)

Publication dâ€™une API Databricks pour accÃ¨s temps rÃ©el

## ğŸ‘©â€ğŸ’» Auteur

HÃ©lÃ¨ne Dubourg
Alternante IngÃ©nieur Data

Curieuse, rigoureuse et passionnÃ©e par les pipelines bien ficelÃ©s.
Â« Parce quâ€™une donnÃ©e propre, câ€™est une donnÃ©e heureuse. Â»
