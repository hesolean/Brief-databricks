# ğŸ’§ Pipeline Azure Databricks â€“ Analyse de la QualitÃ© de lâ€™Eau en France  

> Un pipeline qui coule de source ğŸ’§  
> Projet rÃ©alisÃ© dans le cadre dâ€™un brief dâ€™alternance en **Data Engineering**.

[![Build Status](https://github.com/hesolean/Brief-databricks/actions/workflows/release.yml/badge.svg)](https://github.com/<ton_user>/<ton_repo>/actions)
[![semantic-release](https://img.shields.io/badge/semantic--release-automated-brightgreen)](https://github.com/semantic-release/semantic-release)

---

## ğŸ“˜ Contexte du projet

Ce projet vise Ã  concevoir un **pipeline de donnÃ©es moderne** sur **Azure Databricks**, suivant lâ€™architecture **Medallion (Bronze â€“ Silver â€“ Gold)**, pour analyser la **qualitÃ© de lâ€™eau potable en France** Ã  partir des donnÃ©es publiques de [data.gouv.fr](https://www.data.gouv.fr).

Les contrÃ´les sanitaires de lâ€™**ANSES** (Agence Nationale de SÃ©curitÃ© Sanitaire) servent de base Ã  ce pipeline.  
Il permet dâ€™**ingÃ©rer**, **transformer** et **analyser** les rÃ©sultats de conformitÃ© de lâ€™eau distribuÃ©e commune par commune.

---

## ğŸ¯ Objectifs du projet

### Objectif principal
Construire un pipeline complet sur **Azure Databricks** avec **Delta Live Tables (DLT)** pour automatiser le traitement des donnÃ©es selon lâ€™architecture **Medallion**.

### Objectifs secondaires
- âš™ï¸ **Infrastructure Azure :** Configurer Azure Data Lake Storage Gen2 et Databricks  
- ğŸš° **Ingestion automatisÃ©e :** ImplÃ©menter DLT pour ingÃ©rer les donnÃ©es brutes  
- ğŸ” **CI/CD :** Mettre en place un workflow GitHub Actions avec **Semantic Release**  
- ğŸ§ª **QualitÃ© des donnÃ©es :** *(prÃ©vu)* IntÃ©gration de **Great Expectations**  
- ğŸ“† **Orchestration :** *(prÃ©vu)* Automatisation via Databricks Workflows  
- ğŸŒ **API :** *(prÃ©vu)* Exposition des donnÃ©es via lâ€™API Databricks  

> ğŸ’¡ Le projet est finalisÃ© jusquâ€™Ã  la mise en place de la CI/CD avec Semantic Release.  
> Les Ã©tapes suivantes sont planifiÃ©es comme Ã©volutions.

---

## ğŸ§± Structure du projet

```bash
pipeline-qualite-eau/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ ci.yml
â”‚       â””â”€â”€ release.yml
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 1-Import donnÃ©es source.ipynb
â”‚   â”œâ”€â”€ 2-Initier la couche bronze.ipynb
â”‚   â””â”€â”€ 3-Initier la couche silver.ipynb
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .releaserc.json
â”œâ”€â”€ package.json
â”œâ”€â”€ CHANGELOG.md
â”œâ”€â”€ Brief Databrick.odt
â””â”€â”€ formation-data/
```
---

## ğŸ““ Description des notebooks
### 1ï¸âƒ£ Import donnÃ©es source

Ce notebook gÃ¨re lâ€™ingestion initiale des fichiers .txt depuis un Azure Blob Storage.
Les identifiants du stockage sont transmis au cluster via des variables dâ€™environnement pour Ã©viter toute exposition de secrets.
RÃ©sultat : une table de travail brute, prÃªte Ã  Ãªtre exploitÃ©e.

### 2ï¸âƒ£ Initier la couche Bronze

Ce notebook crÃ©e la couche Bronze avec Delta Live Tables (DLT).
Les donnÃ©es y sont stockÃ©es telles quâ€™elles sont reÃ§ues, sans transformation, pour prÃ©server une trace intÃ©grale des sources.

### 3ï¸âƒ£ Initier la couche Silver

Ce notebook dÃ©finit la couche Silver, oÃ¹ les donnÃ©es sont nettoyÃ©es, normalisÃ©es et enrichies.
Une colonne conformite est ajoutÃ©e :

non si le texte "non conforme" est prÃ©sent dans conclusionprel,

oui sinon.

Cette couche prÃ©pare les donnÃ©es pour la future analyse (couche Gold).

## ğŸ” IntÃ©gration Continue & Semantic Release

Une pipeline GitHub Actions est configurÃ©e pour gÃ©rer automatiquement les versions du projet via Semantic Release.

### âš™ï¸ Fonctionnement

Workflow : .github/workflows/release.yml

DÃ©clenchement : Ã  chaque push sur main ou dev

Versioning automatique : basÃ© sur le format Conventional Commits (feat:, fix:, docs:, etc.)

Changelog gÃ©nÃ©rÃ© : mise Ã  jour automatique de CHANGELOG.md

Tag Git : crÃ©Ã© automatiquement Ã  chaque release

### ğŸ§© Commande locale
npx semantic-release

### âœ… Avantages

Gestion de version automatisÃ©e et cohÃ©rente

Changelog toujours Ã  jour

Transparence sur lâ€™Ã©volution du projet

Compatible avec les workflows de dÃ©ploiement CI/CD

## ğŸ§  Environnement technique
Outil / Service	RÃ´le
Azure Databricks	ExÃ©cution des notebooks et pipelines
Azure Data Lake Gen2	Stockage des donnÃ©es brutes et transformÃ©es
DLT (Delta Live Tables)	Gestion automatisÃ©e des tables Bronze / Silver
GitHub Actions	IntÃ©gration continue et automatisation des versions
Semantic Release	Gestion sÃ©mantique du versioning et changelog
ğŸ“¸ Documentation et suivi

Le fichier Brief Databrick.odt contient :

des captures dâ€™Ã©cran des notebooks et pipelines,

la configuration des clusters Databricks,

et des notes de travail sur le dÃ©roulÃ© du projet.

## ğŸš€ Ã‰volutions prÃ©vues

CrÃ©ation de la couche Gold (donnÃ©es prÃªtes pour la visualisation)

Mise en place de tests de qualitÃ© avec Great Expectations

Orchestration complÃ¨te via Databricks Workflows

Publication dâ€™une API Databricks pour accÃ¨s temps rÃ©el

## ğŸ‘©â€ğŸ’» Auteur

HÃ©lÃ¨ne Dubourg
Alternante IngÃ©nieur Data

Curieuse, rigoureuse et passionnÃ©e par les pipelines bien ficelÃ©s.
Â« Parce quâ€™une donnÃ©e propre, câ€™est une donnÃ©e heureuse. Â»
